{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Intro to JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179084fd",
   "metadata": {},
   "source": [
    "Author: Adrian Price-Whelan (CCA, 2025)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5946975c",
   "metadata": {},
   "source": [
    "[Live notebook on Colab](https://colab.research.google.com/github/JAXtronomy/tutorials/blob/main/tutorials/Intro-to-JAX.ipynb)\n",
    "\n",
    "[Here is the slide deck](../../_static/Intro-to-JAX-for-astronomers.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%xmode minimal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## JIT Compilation\n",
    "\n",
    "Just-in-Time (JIT) compilation enables getting low-level language performance from Python code, on the fly. This can dramatically speed up some numerical computations, especially where loops are required or where the code is not easily vectorized. \n",
    "\n",
    "### Example: Computing the gravitational potential of a set of particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x, a):\n",
    "    return jnp.log(jnp.sum(x**2, axis=1) + a**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.normal(size=(1_000_000, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# without JIT:\n",
    "%timeit func(x, 1.0).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "func_jit = jax.jit(func)\n",
    "\n",
    "# this first call compiles the function\n",
    "func_jit(x, 1.0).block_until_ready()\n",
    "\n",
    "# with JIT:\n",
    "%timeit func_jit(x, 1.0).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "This example is a little contrived since the non-JITted version is already vectorized, but it serves to illustrate the JIT compilation process. JIT is most useful when you need to run a function call many times either in a loop or some other iterative process. For example, optimization, MCMC sampling, orbit integration, etc.\n",
    "\n",
    "BTW: Other packages like Numba and PyTorch also have JIT compilation, but JAX's JIT pairs with XLA to produce highly optimized and hardware-specific machine code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "A brief aside about `.block_until_ready()`: JAX uses lazy evaluation, so it doesn't actually run the code until you need the values. This is great for performance, because JAX will build up a graph of function calls, compile it and send it to XLA for further optimization. But it can be confusing when debugging -- if you don't do this, it can seem like your code is running absurdly fast. `.block_until_ready()` forces JAX to run the code and wait for it to finish before continuing. Compare these:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Vectorization with `vmap`\n",
    "\n",
    "If your function can be expressed using standard `numpy` operations, writing JAX code with `jax.numpy` to accept arrays as inputs will automatically vectorize the function. However, we often write functions that are not easily vectorized - for example, functions that contain conditional statements or loops. In these cases, `jax.vmap` provides an efficient way to apply a function across batches of inputs, often avoiding explicit (external) loops.\n",
    "\n",
    "For example, imagine you have a bunch of stellar spectra for a set of stars, and you want to compute the depth of some absorption line that appears at a different location (in pixels) for each star because of doppler shifts. You could write a function that computes the depth of the line for a single spectrum, and then use `jax.vmap` to apply it to all the stars in your dataset. \n",
    "\n",
    "### Example: Find the depth of an absorption line in a set of spectra\n",
    "\n",
    "First we simulate some \"spectra\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 128  # number of spectra to make\n",
    "\n",
    "rng = np.random.default_rng(123)\n",
    "pix = jnp.arange(100)\n",
    "\n",
    "true_ctr = rng.uniform(40, 60, size=(N,))\n",
    "true_depth = rng.uniform(0.1, 0.5, size=(N,))\n",
    "\n",
    "\n",
    "def make_spectrum(pix, ctr, depth, scale):\n",
    "    return 1 - depth * jnp.exp(-0.5 * (pix - ctr) ** 2 / scale**2)\n",
    "\n",
    "\n",
    "# we'll use vmap to simulate the spectra too ^_^\n",
    "spectra = jax.vmap(\n",
    "    make_spectrum,\n",
    "    in_axes=(None, 0, 0, None),\n",
    "    out_axes=0,\n",
    ")(pix, true_ctr, true_depth, 10.0).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "_ = plt.plot(spectra.T, \"-\", alpha=0.1, color=\"k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "Now we have a set of spectra, and we want to find the depth of the absorption line near pixel ~50. We can write a function that computes the depth of the line for a single spectrum, and then use `jax.vmap` to apply it to all the spectra in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_depth(spectrum):\n",
    "    # find the minimum pixel value:\n",
    "    min_pix = jnp.argmin(spectrum)\n",
    "\n",
    "    # locally fit a parabola to the spectrum to find the inter-pixel minimum\n",
    "    y = jax.lax.dynamic_slice(spectrum, (min_pix - 1,), (3,))\n",
    "    x = jnp.arange(-1, 2)\n",
    "    A = jnp.vander(x, 3, increasing=True)  # creates a matrix with columns [1, x, x**2]\n",
    "    coeffs = jnp.linalg.lstsq(A, y, rcond=None)[0]\n",
    "\n",
    "    return 1 - coeffs[0]\n",
    "\n",
    "\n",
    "find_depth_vmap = jax.vmap(find_depth, in_axes=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for spectrum in spectra:\n",
    "    find_depth(spectrum).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "find_depth_vmap(spectra).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = find_depth_vmap(spectra).block_until_ready()\n",
    "plt.scatter(depths, true_depth)\n",
    "plt.xlabel(\"Measured depth\")\n",
    "plt.ylabel(\"True depth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Automatic Differentiation\n",
    "\n",
    "Automatic differentiation (autodiff or autograd) allows you to compute exact gradients of functions. This is generally most useful for simplifying optimization problems (and enabling faster or novel optimization methods).\n",
    "\n",
    "### Example: Evaluating the gradient of a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def some_func(x, A, P):\n",
    "    return A * jnp.cos(2 * jnp.pi * x / P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_func_grad = jax.grad(some_func, argnums=0)\n",
    "\n",
    "xgrid = jnp.linspace(0, 10, 4096)\n",
    "some_func_grad(xgrid, 1.0, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "Huh, what happened? Why did this error? `jax.grad` only works on scalar outputs. If we try to pass in an array, the output of `some_func` will be an array. Instead of the above, if we want to compute the gradient for all elements in an array, we can use `jax.vmap` to vectorize the `grad`'ed function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_func_grad_vmap = jax.vmap(jax.grad(some_func, argnums=0), in_axes=(0, None, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(xgrid, some_func(xgrid, 1.0, 1.0), \"-\", label=\"f(x)\")\n",
    "plt.plot(xgrid, some_func_grad_vmap(xgrid, 1.0, 1.0), \"-\", label=\"df/dx\")\n",
    "plt.legend(loc=\"lower right\", fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Pytrees\n",
    "\n",
    "Pytrees are JAXâ€™s structured data containers. Think of these as dictionaries, where the data may be scalar or array valued and could be arranged hierarchically. JAX functions often take Pytrees as input and output, and they are designed to be compatible with JAX's JIT compilation and autodiff features. Pytrees are one of my favorite features of JAX!\n",
    "\n",
    "### Example: Computing the gradient of a function with respect to a set of parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x, a, b):\n",
    "    return a * x + b\n",
    "\n",
    "\n",
    "def objective(params, x, y, y_err):\n",
    "    model_y = model(x, params[\"a\"], params[\"b\"])\n",
    "    return jnp.sum((y - model_y) ** 2 / y_err**2)  # chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(42)\n",
    "x = rng.uniform(0, 10, size=16)\n",
    "y = model(x, a=8.67, b=5.309)\n",
    "y_err = rng.uniform(0.1, 5.0, size=len(x))\n",
    "y += rng.normal(0, y_err, size=len(x))\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.errorbar(x, y, yerr=y_err, fmt=\"o\", label=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "objecive_grad = jax.grad(objective, argnums=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "objecive_grad({\"a\": 1.0, \"b\": 1.0}, x, y, y_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### Example: Using `vmap` over a pytree of parameters\n",
    "\n",
    "You can use `jax.vmap` to efficiently evaluate a function for a batch of pytree keys. Suppose we have arrays of parameter sets and we want to evaluate the function for each set of parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_arr = {\n",
    "    \"a\": jnp.linspace(0, 10, 5),\n",
    "    \"b\": jnp.linspace(-5, 5, 5),\n",
    "}\n",
    "\n",
    "objective_vmap = jax.vmap(objective, in_axes=({\"a\": 0, \"b\": 0}, None, None, None))\n",
    "chi2_arr = objective_vmap(params_arr, x, y, y_err).block_until_ready()\n",
    "chi2_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## Sharp Bits\n",
    "\n",
    "### Control flow\n",
    "\n",
    "In general, use `jax.lax.cond` instead of `if` statements, and `jax.lax.scan` instead of `for` loops. This is because JAX uses XLA to compile the code, and XLA needs to know the shape of the data at compile time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def func_if1(x):\n",
    "    if x > 0:\n",
    "        return x**3\n",
    "    else:\n",
    "        return x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "func_if1(10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def func_if2(x):\n",
    "    return jax.lax.cond(x > 0, lambda: x**3, lambda: x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "func_if2(10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "## Teaser: Using physical units in JAX code\n",
    "\n",
    "`astropy.units` has the `Quantity` object for handling data with associated physical units. However, this is an explicit subclass of `numpy.ndarray`, so JAX does not work natively with Astropy units. Instead, in the meantime, we are building a new package called `unxt` for handling JAX tracers with physical units. This package is still in development, but it is ready for use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unxt as u\n",
    "import quaxed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = u.Quantity(jnp.arange(10.0), \"kpc\")\n",
    "time = u.Quantity(5.3, \"Gyr\")\n",
    "\n",
    "result = pos / time\n",
    "result.unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_potential(r, GM, a):\n",
    "    # Gravitational potential for a Hernquist model:\n",
    "    return -GM / (r + a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "dPhi_dr = quaxed.grad(compute_potential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = u.Quantity(1.0, \"kpc\")\n",
    "GM = u.Quantity(1.0, \"kpc^3 / Gyr^2\")\n",
    "a = u.Quantity(1.0, \"kpc\")\n",
    "compute_potential(r, GM, a).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "dPhi_dr(r, GM, a).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
